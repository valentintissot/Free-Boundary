\section{Neural Parametrization}

\subsection{Representation of the Free Boundary}

In free boundary problems, 
the region sought can be represented locally, up to a rotation of axes, as the epigraph of a threshold function $f: [0,T]\times \calX \to \R$, namely
$$\calS_t = \{x\in \calX \,|\, x_{\bar{d}} \ge f(t,x_{-})\},$$
where $\bar{d}= \text{dim}(\calX)= d+m+k$,  $x_{-}=(x_1,...,x_{\bar{d}-1})$. 
The free boundary is thus formed by the points $x\in \calX$ satisfying  $x_{\bar{d}} = f(t,x_{-})$. We generalize this representation by considering other "separations" of $\calX$ as explained in the following assumption.  

\begin{asm}
\label{asm:star}
There exists a homeomorphism $A=(\alpha,\Xi) : \calX  \to \calA \times \calE$, $\calA = \alpha(\calX) \subseteq \R$, $\calE = \Xi(\calX)$  
satisfying the following property: If $x,x' \in \calX$ such that  $\Xi(x')=\Xi(x)$, then for $t\in [0,T]$, 
\begin{equation*}
   x \in \calS_t \ \textnormal{ and } \ \ \alpha(x') \ge \alpha(x)  \; \Longrightarrow  \;
x' \in \calS_t.
\end{equation*}
\end{asm} 

In light of \cref{asm:star},  we define the \textit{threshold function} $
f: [0,T]\times \calE \to \R$  by 

\begin{equation}\label{eq:thres}
   f(t,\xi) =
\inf \calA_{t,\xi}, \quad \calA_{t,\xi} = \{a \in \calA \ | \  A^{-1}(a,\xi)\in \calS_t\}. 
\end{equation}
The following result explains how the threshold function relates to the stopping region. 
%$The motivation of our approach is reinforced by the following lemma. 
\begin{proposition}\label{prop:thresStop}
Let $f$ as in $\eqref{eq:thres}$ and consider the subset $\frakS_t = \{x\in \calX \ | \ \alpha(x)  \ge f(t,\Xi(x)) \}.$
Then  for each $t\in [0,T]$, the following inclusions hold,
$$\mathring{\frakS}_t \subseteq \calS_t  \subseteq \frakS_t. $$ 
Moreover, if  $v$ is continuous, then $\calS_t = \frakS_t$ and 
$\partial \! \calS_t = \{x\in \calX \ | \ \alpha(x) = f(t,\Xi(x))\}$.
\end{proposition}
\begin{proof}
 First, assume that $x \notin \calS_t$. Then $\alpha(x) \notin \calA_{t,\xi}$ with $\xi := \Xi(x)$. Since $a\in \calA_{t,\xi}$ implies  $a'\in \calA_{t,\xi}$ for all $a' \ge a$, we conclude that  $\alpha(x) \le  \inf \calA_{t,\xi} = f(t,\xi)$ and
 $\mathring{\frakS}_t \subseteq \calS_t$ follows. For the second inclusion, suppose that $x\in \calS_t$. Then $\alpha(x) \in \calA_{t,\Xi(x)}$,  which immediately gives $\alpha(x) \ge f(t,\Xi(x))$. Thus $\calS_t \subseteq \frakS_t$ as well. Finally, when $v$ is continuous, then $\calS_t$ is a closed subset of $\frakS_t$ that contains $\mathring{\frakS}_t$, hence $\calS_t = \frakS_t$.
\end{proof}

We recall that under mild assumptions on $X$,  %(namely that  $X$ is stationary), 
the stopping region expands over time, i.e. $\calS_t \subseteq \calS_{t'}$ whenever $t\le t'$; see \cref{prop:monot}. 
It follows from \cref{prop:thresStop}  that   $t\mapsto f(t,\xi)$ must be non-increasing for all $\xi\in \calE$.

To solve the optimal stopping problem, the above construction suggests to parametrize the threshold function, namely 
\begin{equation}
    f(t,\xi) \approx G(t,\xi; \theta), \quad G(\cdot,\cdot; \theta) \in \calG_p, 
\end{equation}
with $\calG_p = \{G(\cdot,\cdot; \theta) \ | \ \theta \in \Theta_p \}$ and compact parameter subset $\Theta_p \subseteq \R^p$, $p\in \N$. This gives, in turn, the parametrized free boundary 
$\Gamma_t^{\theta} = \{x \in \mathring{\calX} \ | \ \alpha(x) = G(t,\Xi(x); \theta)\}. $ 
The stopping time associated to $\theta \in \Theta_d$ is therefore
\begin{equation}\label{eq:stopTime}
    \tau^{\theta} = \inf\{t \in [0,T] \ | \ \alpha(X_t) \ge G(t,\Xi(X_t); \theta) \} \wedge T,
\end{equation}
with associated reward  $\calR(\theta)$ as defined  in $\eqref{eq:reward}$.

\subsection{Deep Monte Carlo Optimization} \label{sec:DMCO}
We adapt the \textit{Deep Monte Carlo Optimization} algorithm \cite{HanJentzenE} to our framework. Let $\calG_p$ be a family of feedforward neural networks  with a total of $p$ parameters. If the optimal threshold function $f$ is continuous, then the universal approximation property of neural networks  \cite{Cybenko,Hornik} guarantees the existence of $p\in \N$ and parameter $\theta \in \Theta_p$ such that $G(\cdot,\cdot;\theta)$ is arbitrarily close to $f$.  

To find a near-optimal parameter vector, we employ  \textit{stochastic gradient ascent} (SGA), which we explain now. First, we discretize time along a partition $\Pi_N = \{0 = t_0 < \ldots < t_N=T\}$, $N\in \N$. Therefore, the stopping times $\tau^{\theta}$ in $\eqref{eq:stopTime}$ take value in   $\calT_N = \calT \cap \ \Pi_N$. %\calT_N := 
If there are finitely many exercise date, we simply set $\Pi_N = \calT$, i.e. $\calT_N = \calT$. 
Second, the reward function $\calR$ in $\eqref{eq:reward}$ is replaced by its empirical counterpart, namely
\begin{equation}\label{eq:empReward1}
    \hat{\calR}_B(\theta) = \frac{1}{B}\sum_{j=1}^B   D_{0,\tau^{\theta,j}}  \ \varphi(X_{\tau^{\theta,j}}^j),
\end{equation}
where $B\in \N$ is the \textit{batch size} and  $\tau^{\theta,j}$ denotes the realized  stopping time as in $\eqref{eq:stopTime}$ for the $j-$th trajectory. Next, 
starting with initial weights  $\theta^{(0)}\in \Theta_p$ (e.g. generated randomly), we iteratively update the parameter vector  according to 
\begin{equation}\label{eq:SGA}
\theta^{(m)} = \theta^{(m-1)} + \zeta_m \nabla \hat{\calR}(\theta^{(m-1)}), \quad m=1,\ldots, M,  
\end{equation}
with \textit{learning rates} $(\zeta_m)$. 
However, applying the stochastic gradient method as in $\eqref{eq:SGA}$ will not work because the stopping time relies on a binary decision.  Indeed, assuming that the distribution of $X_t$ is atomless and $\alpha$ is nowhere constant, then an infinitesimal shift of the boundary will not alter the sign of  $\alpha(X^{j}_{t_n}) - G(t_n,\Xi(X^{j}_{t_n}); \theta)$. In turn, the corresponding stopping time and  reward will remain unchanged. 
Hence, $\nabla \hat{\calR}$ vanishes and the SGA method fails to  converge to a (local) maximum. 
The remedy is to relax the stopping decision, as  explained in the next section. 
\subsection{Fuzzy Boundary and Relaxed Stopping}

\begin{algorithm}[t]
\label{alg:FBTrain}
\caption{Free Boundary Training}\label{alg:cap} 
\begin{enumerate}
\setlength \itemsep{0.05ex}
\vspace{-3mm}

\item \textbf{Initialize} $\theta^{0} \in \Theta_p$
\item \textbf{For} $m = 0,\ldots, M-1$
    \begin{itemize}
    \setlength \itemsep{0.4ex}
    \vspace{-2mm}
        \item \textbf{Simulate} trajectories $(X_{t_n}^j)_{n=0}^N$, $\ j=1,\ldots,B$.
        \item 
        \textbf{Compute} the
        \vspace{-3mm}
\begin{align*}
-&\text{ signed distances:} \quad &\delta^{\theta^{m}, j}_n &=  G(t_n,\Xi(X^j_{t_n}),\theta^{m}) \ - \ \alpha(X^j_{t_n}) \\[1em]
-&\text{ stopping factors:}\quad &p^{\theta^{m},\epsilon,j}_{n} &= h(\delta^{\theta^{m}, j}_n/\epsilon) \\[1em]
 -&\text{ stopping probabilities:}\quad &P^{\theta^{m},\epsilon,j}_{n} &= p^{\theta^{m},\epsilon,j}_{n}(1-q^{\theta^{m},\epsilon,j}_{n}) \quad  \text{($q^{\theta^{m},\epsilon,j}_{n}$ as in $\eqref{eq:recStop}$)}\\[0.2em]
    -&\text{ reward function:}\quad &\hat{\calR}_B^{\epsilon}(\theta^{m}) &=  \frac{1}{B}\sum_{j=1}^B  \sum_{n=0}^N D_{0,t_n}   P^{\theta^{m},\epsilon,j}_{n}\ \varphi(X_{t_n}^j)
    \end{align*}
          \vspace{-4mm}
          
        \item \textbf{Update:} $\theta^{m+1} = \theta^{m} + \zeta_m \nabla \hat{\calR}_B^{\epsilon}(\theta^{m})$
    \end{itemize}
\item \textbf{Return} $\theta^{M}$
\vspace{-3mm}

\end{enumerate}
\end{algorithm}

Instead of a sharp interface, we introduce a region,  called the \textit{fuzzy boundary}, where the binary decision ($1:$  stop, $0:$ continue) is replaced by a value in $(0,1)$. 
To this end,  we compute the signed distances to the boundary, namely
 \begin{equation}
     \delta^{\theta}_n =  G(t_n,\Xi(X_{t_n}),\theta) \ - \ \alpha(X_{t_n}), \quad n=0,\ldots ,N-1. 
 \end{equation}
 These are converted into \textit{stopping factors}  
$p^{\theta,\epsilon}_n := h(\delta^{\theta}_n /\epsilon), $ $\epsilon \ge 0$, where $h:\R \to [0,1]$ is a continuous function such that $h \equiv 1$ on $(-\infty,-1]$,  $h \equiv 0$ on $[1,\infty)$ and decreasing in $(-1,1)$. One can take for instance $h(\delta) = \frac{(1-\delta)^+ }{2} \wedge 1$. This gives the following  relaxed stopping decision for each $n\in\{0,\ldots,N-1\}$, 
$$ \begin{cases}
p^{\theta,\epsilon}_n = 1, & \text{ if } \  \delta^j_n \le -\epsilon,  \quad \text{(stopping region)}\\%ping region
p^{\theta,\epsilon}_n \in (0,1), & \text{ if } \   |\delta_n^{j}| < \epsilon, \hspace{0.4 mm} \quad \text{(fuzzy boundary)}\\
p^{\theta,\epsilon}_n = 0, & \text{ if } \  \delta^j_n \ge \epsilon. 
\hspace{3.5 mm} \quad \text{(continuation region)}
\end{cases} $$
At maturity, i.e.  $n=N$, we can simply set $p^{\theta,\epsilon}_N \equiv 1$. 
The hyperparameter $\epsilon$ thus determines the "width" of the fuzzy boundary. 

Thereafter, let $q^{\theta,\epsilon}_{n} \in [0,1]$ be the probability that the option has been exercised before time $t_n$, i.e. $\tau^{\theta} < t_n$. Then the values of $q^{\theta,\epsilon}_{n}$  are obtained thanks to the recursive formula, 
\begin{align}\label{eq:recStop}
     q^{\theta,\epsilon}_{0} = 0, \quad 
     q^{\theta,\epsilon}_{n+1}
     = \underbrace{q^{\theta,\epsilon}_{n}}_{ \tau^{\theta} < t_n} + \underbrace{p^{\theta,\epsilon}_{n}(1-q^{\theta,\epsilon}_{n})}_{\tau^{\theta} = t_n}, \quad n = 0, \ldots, N-1. 
\end{align}
Clearly, $n \mapsto q^{\theta,\epsilon}_{n}$ is non-decreasing and equal to $1$ if $X$ strictly enters the stopping region, i.e.  $\delta^j_n \le -\epsilon$.   
The probability of stopping at time $t_n \in \calT_{\!N}$  is thus $P^{\theta,\epsilon}_{n} := p^{\theta,\epsilon}_{n}(1-q^{\theta,\epsilon}_{n})$. Finally,  the  reward obtained with the  fuzzy boundary is given by 

\begin{equation}\label{eq:empReward2}
\hat{\calR}_B^{\epsilon}(\theta) =  \frac{1}{B}\sum_{j=1}^B  \sum_{n=0}^N D_{0,t_n}   P^{\theta,\epsilon,j}_{n}\ \varphi(X_{t_n}^j),
\end{equation}
and $\eqref{eq:empReward1}$ is recovered by letting $\epsilon  \downarrow 0$. 
The training phase is summarized in \cref{alg:cap}. To compute the initial value of the optimal stopping problem, we choose a large number of simulations $J\in \N$ and use the sharp boundary formulation $\eqref{eq:empReward1}$, i.e.   
$ \hat{v}_0 = \hat{\calR}_J(\theta^{(M)}) .$ 
 \begin{remark}
 Note that the value function is also  available at any intermediate time $t\in \calT_{\!N}$ once the threshold function has been trained. Indeed, one  can set 
 $\hat{v}(t,s) = \frac{1}{J}\sum_{j=1}^J   D_{t,\tau_t^{\theta,j}}  \ \varphi(X_{\tau_t^{\theta,j}}^j),$ 
 with $$\tau_t^{\theta,j} = \inf\{u \in \calT_{N} \cap \, [t,T]\ | \ \alpha(X_u) \ge G(u,\Xi(X_u); \theta) \} \wedge T.$$
 \end{remark}
 
\subsection{Importance Sampling}
To properly train the threshold function in \cref{alg:cap}, 
it is  crucial  that the simulated paths $\alpha(X^{j})$ hit the (fuzzy) boundary frequently enough and  across exercise dates. 
 
This can be achieved using importance sampling, as explained below.  

Let $W$ be a $d-$dimensional Brownian motion under $\Q$ and suppose that $S$ evolves according to the stochastic differential equation, 
\begin{align}\label{eq:stockSDE}
    dS_t = \mu_t dt + \sigma_t dW_t, \quad S_0 \in \R,
\end{align}
where $\mu$ (resp. $\sigma$) is a  $d-$dimensional  (resp. $\R^{d\times d}-$valued) process that may depend on $t,S_t$ and other exogenous factors. 
Then consider the Girsanov transformation 
\begin{equation}\label{eq:Girsanov}
\frac{d\Q^{\lambda}}{d\Q} = \calE_T(- \lambda \bullet W), \quad \lambda  \text{ predictable},\quad   \E^{\Q}\left[\exp\left(\frac{1}{2}\lVert \lambda\rVert^2_{L^2([0,T];\R^d)}\right)\right] <\infty, 
\end{equation}
so that 
$W_t^{\lambda} :=  \int_0^{t}\lambda_s ds + W_t$ is Brownian motion under $\Q^{\lambda}$. Therefore, $S$ has $\Q^{\lambda}-$dynamics
\begin{equation}
    dS_t = \mu^{\lambda}_t dt + \sigma_t dW^{\lambda}_t,\quad \mu^{\lambda}_t = \mu_t - \sigma_t \lambda_t. 
\end{equation}
If we define $Z^{\lambda}_t = \frac{d\Q}{d\Q^{\lambda}} \big |_{\calF_t}  = \calE_t(\lambda \bullet W^{\lambda})$, then the expected reward associated to some $\tau \in \vartheta(\calT)$ reads  $\E^{\Q}[D_{0,\tau}\, \varphi(X_{\tau})] =  \E^{\Q^{\lambda}}[Z^{\lambda}_{\tau}D_{0,\tau}\, \varphi(X_{\tau})]. $ 
In turn, 
the empirical reward $\eqref{eq:empReward2}$ becomes 
\begin{equation}\label{eq:empReward3}
\hat{\calR}_B^{\epsilon,\lambda}(\theta) =  \frac{1}{B}\sum_{j=1}^B  \sum_{n=0}^N Z^{\lambda,j}_{t_n} D_{0,t_n}   P^{\theta,\epsilon,j}_{n}\ \varphi(X_{t_n}^j).
\end{equation}
The process $\lambda$ can be chosen so as to increase the probability of crossing the boundary before maturity. In the numerical experiments, the process $\lambda$ is assumed to be constant for simplicity; see for instance  \cref{sec:putBS,sec:maxCallSym}. Therefore, $\lambda$ is in this case seen as a hyperparameter vector. 