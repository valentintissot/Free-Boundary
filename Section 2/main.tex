\section{Neural Parametrization}

\subsection{Representation of the Free Boundary}

In free boundary problems, %it is common to represent
the region sought can be represented locally, up to a rotation of axes, as the epigraph of a threshold function $f: [0,T]\times \calX \to \R$, namely
$$\calS_t = \{x\in \calX \,|\, x_{\bar{d}} \ge f(t,x_{-})\},$$
where $\bar{d}= \text{dim}(\calX)= d+m+k$,  $x_{-}=(x_1,...,x_{\bar{d}-1})$.
%See for instance \citet{Laurence}. 
The free boundary is thus formed by the points $x\in \calX$ satisfying  $x_{\bar{d}} = f(t,x_{-})$. We generalize this representation by considering other "separations" of $\calX$ as explained in the following assumption. %We make the following general assumption. 

\begin{asm}
\label{asm:star}
There exists a homeomorphism $A=(\alpha,\Xi) : \calX  \to \calA \times \calE$, $\calA = \alpha(\calX) \subseteq \R$, $\calE = \Xi(\calX)$  
% \begin{equation}\label{eq:homeo}
% A:=(\alpha,\Xi) : \calX  \to \calA \times \calE \subseteq \R \times \calX,  
% \end{equation}
%i.e. $\calA = \alpha(\calX), \ \calE = \Xi(\calX)$,  
satisfying the following property: If $x,x' \in \calX$ such that  $\Xi(x')=\Xi(x)$, then for $t\in [0,T]$,% and $s \in \calS_t$ for some $t \in [0,T]$, 
%we have 
\begin{equation*}
   x \in \calS_t \ \textnormal{ and } \ \ \alpha(x') \ge \alpha(x)  \; \Longrightarrow  \;
x' \in \calS_t.
\end{equation*}
\end{asm} 

In light of \cref{asm:star},  we define the \textit{threshold function} $
f: [0,T]\times \calE \to \R$  by 
% \begin{equation}\label{eq:threshold}
%   f(t,\xi) =
% \sup \calA_{t,\xi}, \quad  \calA_{t,\xi}=\{a \ge 0 \ | \ (a,\xi) \in \calZ \text{ and } A^{-1}(a,\xi)\in \calS_t\}. 
% \end{equation}

\begin{equation}\label{eq:thres}
   f(t,\xi) =
\inf \calA_{t,\xi}, \quad \calA_{t,\xi} = \{a \in \calA \ | \  A^{-1}(a,\xi)\in \calS_t\}. 
\end{equation}
The following result explains how the threshold function relates to the stopping region. 
%$The motivation of our approach is reinforced by the following lemma. 
\begin{proposition}\label{prop:thresStop}
Let $f$ as in $\eqref{eq:thres}$ and consider the subset $\frakS_t = \{x\in \calX \ | \ \alpha(x)  \ge f(t,\Xi(x)) \}.$
Then  for each $t\in [0,T]$, the following inclusions hold,
$$\mathring{\frakS}_t \subseteq \calS_t  \subseteq \frakS_t. $$ 
Moreover, if  $v$ is continuous, then $\calS_t = \frakS_t$ and 
$\partial \! \calS_t = \{x\in \calX \ | \ \alpha(x) = f(t,\Xi(x))\}$.
\end{proposition}
\begin{proof}
 First, assume that $x \notin \calS_t$. Then $\alpha(x) \notin \calA_{t,\xi}$ with $\xi := \Xi(x)$. Since $a\in \calA_{t,\xi}$ implies  $a'\in \calA_{t,\xi}$ for all $a' \ge a$, we conclude that  $\alpha(x) \le  \inf \calA_{t,\xi} = f(t,\xi)$ and
 $\mathring{\frakS}_t \subseteq \calS_t$ follows. For the second inclusion, suppose that $x\in \calS_t$. Then $\alpha(x) \in \calA_{t,\Xi(x)}$,  which immediately gives $\alpha(x) \ge f(t,\Xi(x))$. Thus $\calS_t \subseteq \frakS_t$ as well. Finally, when $v$ is continuous, then $\calS_t$ is a closed subset of $\frakS_t$ that contains $\mathring{\frakS}_t$, hence $\calS_t = \frakS_t$.
\end{proof}

We recall that under mild assumptions on $X$,  %(namely that  $X$ is stationary), 
the stopping region expands over time, i.e. $\calS_t \subseteq \calS_{t'}$ whenever $t\le t'$; see \cref{prop:monot}. 
It follows from \cref{prop:thresStop}  that   $t\mapsto f(t,\xi)$ must be non-increasing for all $\xi\in \calE$.  %; see \cref{prop:monot} and \cref{cor:monot}. 

To solve the optimal stopping problem, the above construction suggests to parametrize the threshold function, namely 
\begin{equation}
    f(t,\xi) \approx G(t,\xi; \theta), \quad G(\cdot,\cdot; \theta) \in \calG_p, 
\end{equation}
with $\calG_p = \{G(\cdot,\cdot; \theta) \ | \ \theta \in \Theta_p \}$ and compact parameter subset $\Theta_p \subseteq \R^p$, $p\in \N$. This gives, in turn, the parametrized free boundary 
$\Gamma_t^{\theta} = \{x \in \mathring{\calX} \ | \ \alpha(x) = G(t,\Xi(x); \theta)\}. $ 
The stopping time associated to $\theta \in \Theta_d$ is therefore
\begin{equation}\label{eq:stopTime}
    \tau^{\theta} = \inf\{t \in [0,T] \ | \ \alpha(X_t) \ge G(t,\Xi(X_t); \theta) \} \wedge T,
\end{equation}
with associated reward  $\calR(\theta)$ as defined  in $\eqref{eq:reward}$.

% \begin{example}\label{ex:1DExample2}
% Consider the standard American call option example discussed in \cref{sec:1DExample}. Then \cref{asm:star} is fulfilled with the homeomorphism $A = (\alpha,\Xi)$ given by $\alpha(s) = s$ and $\Xi \equiv 0$. Indeed, we have seen in \cref{sec:1DExample} that $\calS_t$ is a right-unbounded interval, hence $s\in \calS_t$ and  $s' = \alpha(s') \ge \alpha(s) =s$ implies that $s'\in \calS_t$ as well.  As $\Xi$ is constant, the threshold function $G(\cdot;\theta)$ depends solely on time for fixed $\theta$. Therefore, the stopping time in $\eqref{eq:stopTime}$ simply reads $$\tau^{\theta} = \inf\{t \in [0,T] \ | \ S_t \ge G(t; \theta) \} \wedge T.$$
% %$$v(t,s') $$  %$\calS = \prod_{t\in [0,T]} [f(t), \infty), $ for some threshold function $f:[0,T]\to [K,\infty)$.  

% %Indeed, if $s\in \calS_t$ and $s' = \alpha(s') \ge \alpha(s) =s$, then 
% %$$v(t,s') $$

% \end{example}

\subsection{Deep Monte Carlo Optimization} \label{sec:DMCO}
We adapt the \textit{Deep Monte Carlo Optimization} algorithm \cite{HanJentzenE} to our framework. Let $\calG_p$ be a family of feedforward neural networks  with a total of $p$ parameters. If the optimal threshold function $f$ is continuous, then the universal approximation property of neural networks  \cite{Cybenko,Hornik} guarantees the existence of $p\in \N$ and parameter $\theta \in \Theta_p$ such that $G(\cdot,\cdot;\theta)$ is arbitrarily close to $f$.  %In particular, if $v_0^p$ is the value function as in $\eqref{eq:paramValue}$, we expect that  $v_0^p \to v(0,\cdot)$ as $p \to \infty$. 

To find a near-optimal parameter vector, we employ  \textit{stochastic gradient ascent} (SGA), which we explain now. First, we discretize time along a partition $\Pi_N = \{0 = t_0 < \ldots < t_N=T\}$, $N\in \N$. Therefore, the stopping times $\tau^{\theta}$ in $\eqref{eq:stopTime}$ take value in   $\calT_N = \calT \cap \ \Pi_N$. %\calT_N := 
If there are finitely many exercise date, we simply set $\Pi_N = \calT$, i.e. $\calT_N = \calT$. 
Second, the reward function $\calR$ in $\eqref{eq:reward}$ is replaced by its empirical counterpart, namely
\begin{equation}\label{eq:empReward1}
    \hat{\calR}_B(\theta) = \frac{1}{B}\sum_{j=1}^B   D_{0,\tau^{\theta,j}}  \ \varphi(X_{\tau^{\theta,j}}^j),
\end{equation}
where $B\in \N$ is the \textit{batch size} and  $\tau^{\theta,j}$ denotes the realized  stopping time as in $\eqref{eq:stopTime}$ for the $j-$th trajectory. Next, 
starting with initial weights  $\theta^{(0)}\in \Theta_p$ (e.g. generated randomly), we iteratively update the parameter vector  according to 
\begin{equation}\label{eq:SGA}
\theta^{(m)} = \theta^{(m-1)} + \zeta_m \nabla \hat{\calR}(\theta^{(m-1)}), \quad m=1,\ldots, M,  
\end{equation}
with \textit{learning rates} $(\zeta_m)$. 
%The number of training iterations $M\in \N$ is either fixed or 
However, applying the stochastic gradient method as in $\eqref{eq:SGA}$ will not work because the stopping time relies on a binary decision.  Indeed, assuming that the distribution of $X_t$ is atomless and $\alpha$ is nowhere constant, then an infinitesimal shift of the boundary will not alter the sign of  $\alpha(X^{j}_{t_n}) - G(t_n,\Xi(X^{j}_{t_n}); \theta)$. In turn, the corresponding stopping time and  reward will remain unchanged. 
Hence, $\nabla \hat{\calR}$ vanishes and the SGA method fails to  converge to a (local) maximum. 
The remedy is to relax the stopping decision, as  explained in the next section. % by considering \textit{fuzzy boundaries}

\subsection{Fuzzy Boundary and Relaxed Stopping}

\begin{algorithm}[t]
\label{alg:FBTrain}
\caption{Free Boundary Training}\label{alg:cap} 
\begin{enumerate}
\setlength \itemsep{0.05ex}
\vspace{-3mm}

\item \textbf{Initialize} $\theta^{0} \in \Theta_p$
%\textbf{Given} $\theta^{(0)} \in \Theta$, $\epsilon >0$, $M \in \N$
\item \textbf{For} $m = 0,\ldots, M-1$
    \begin{itemize}
    \setlength \itemsep{0.4ex}
    \vspace{-2mm}
        \item \textbf{Simulate} trajectories $(X_{t_n}^j)_{n=0}^N$, $\ j=1,\ldots,B$.
        \item %\textbf{For} $j = 1,\ldots, B$  
        \textbf{Compute} the
        \vspace{-3mm}
\begin{align*}
-&\text{ signed distances:} \quad &\delta^{\theta^{m}, j}_n &=  G(t_n,\Xi(X^j_{t_n}),\theta^{m}) \ - \ \alpha(X^j_{t_n}) \\[1em]
-&\text{ stopping factors:}\quad &p^{\theta^{m},\epsilon,j}_{n} &= h(\delta^{\theta^{m}, j}_n/\epsilon) \\[1em]
 -&\text{ stopping probabilities:}\quad &P^{\theta^{m},\epsilon,j}_{n} &= p^{\theta^{m},\epsilon,j}_{n}(1-q^{\theta^{m},\epsilon,j}_{n}) \quad  \text{($q^{\theta^{m},\epsilon,j}_{n}$ as in $\eqref{eq:recStop}$)}\\[0.2em]
    -&\text{ reward function:}\quad &\hat{\calR}_B^{\epsilon}(\theta^{m}) &=  \frac{1}{B}\sum_{j=1}^B  \sum_{n=0}^N D_{0,t_n}   P^{\theta^{m},\epsilon,j}_{n}\ \varphi(X_{t_n}^j)
    \end{align*}
%       \begin{itemize}
%       \setlength \itemsep{2ex}
% \item Signed distances: $\delta^{\theta^{m}, j}_n =  G(t_n,\Xi(X^j_{t_n}),\theta^{m}) \ - \ \alpha(X^j_{t_n}) $

% \item Stopping factors: $p^{\theta^{m},\epsilon}_{n} = h(\delta^{\theta^{m}, j}_n/\epsilon) $

%         \item Stopping probabilities: $P^{\theta^{m},\epsilon,j}_{n} = p^{\theta^{m},\epsilon,j}_{n}(1-q^{\theta^{m},\epsilon,j}_{n}) $, with $q^{\theta^{m},\epsilon,j}_{n}$ as in $\eqref{eq:recStop}$
%         \item Reward function: $\hat{\calR}_B^{\epsilon}(\theta^{m}) =  \frac{1}{B}\sum\limits_{j=1}^B  \sum\limits_{n=0}^N D_{0,t_n}   P^{\theta^{m},\epsilon,j}_{n}\ \varphi(X_{t_n}^j)$
%     \end{itemize}
          \vspace{-4mm}
          
        \item \textbf{Update:} $\theta^{m+1} = \theta^{m} + \zeta_m \nabla \hat{\calR}_B^{\epsilon}(\theta^{m})$
    \end{itemize}
\item \textbf{Return} $\theta^{M}$
\vspace{-3mm}

\end{enumerate}
\end{algorithm}

Instead of a sharp interface, we introduce a region,  called the \textit{fuzzy boundary}, where the binary decision ($1:$  stop, $0:$ continue) is replaced by a value in $(0,1)$. %agent chooses to stop with a certain probability. 
To this end,  we compute the signed distances to the boundary, namely
 \begin{equation}
     \delta^{\theta}_n =  G(t_n,\Xi(X_{t_n}),\theta) \ - \ \alpha(X_{t_n}), \quad n=0,\ldots ,N-1. 
 \end{equation}
 These are converted into \textit{stopping factors}  
$p^{\theta,\epsilon}_n := h(\delta^{\theta}_n /\epsilon), $ $\epsilon \ge 0$, where $h:\R \to [0,1]$ is a continuous function such that $h \equiv 1$ on $(-\infty,-1]$,  $h \equiv 0$ on $[1,\infty)$ and decreasing in $(-1,1)$. One can take for instance $h(\delta) = \frac{(1-\delta)^+ }{2} \wedge 1$. This gives the following  relaxed stopping decision for each $n\in\{0,\ldots,N-1\}$, 
$$ \begin{cases}
p^{\theta,\epsilon}_n = 1, & \text{ if } \  \delta^j_n \le -\epsilon,  \quad \text{(stopping region)}\\%ping region
p^{\theta,\epsilon}_n \in (0,1), & \text{ if } \   |\delta_n^{j}| < \epsilon, \hspace{0.4 mm} \quad \text{(fuzzy boundary)}\\
p^{\theta,\epsilon}_n = 0, & \text{ if } \  \delta^j_n \ge \epsilon. 
\hspace{3.5 mm} \quad \text{(continuation region)}%ation region
\end{cases} $$
At maturity, i.e.  $n=N$, we can simply set $p^{\theta,\epsilon}_N \equiv 1$. % as the option has to be exercised. 
The hyperparameter $\epsilon$ thus determines the "width" of the fuzzy boundary. %Moreover, the sharp interface is recovered by letting $\epsilon \downarrow 0$. 

Thereafter, let $q^{\theta,\epsilon}_{n} \in [0,1]$ be the probability that the option has been exercised before time $t_n$, i.e. $\tau^{\theta} < t_n$. Then the values of $q^{\theta,\epsilon}_{n}$  are obtained thanks to the recursive formula, 
\begin{align}\label{eq:recStop}
     q^{\theta,\epsilon}_{0} = 0, \quad 
     q^{\theta,\epsilon}_{n+1}
     = \underbrace{q^{\theta,\epsilon}_{n}}_{ \tau^{\theta} < t_n} + \underbrace{p^{\theta,\epsilon}_{n}(1-q^{\theta,\epsilon}_{n})}_{\tau^{\theta} = t_n}, \quad n = 0, \ldots, N-1. 
\end{align}
Clearly, $n \mapsto q^{\theta,\epsilon}_{n}$ is non-decreasing and equal to $1$ if $X$ strictly enters the stopping region, i.e.  $\delta^j_n \le -\epsilon$.   
The probability of stopping at time $t_n \in \calT_{\!N}$  is thus $P^{\theta,\epsilon}_{n} := p^{\theta,\epsilon}_{n}(1-q^{\theta,\epsilon}_{n})$. Finally,  the  reward obtained with the  fuzzy boundary is given by 
% \begin{enumerate}
% \item $h$ is continuous and non-decreasing,
%     \item $h(\delta) = 0 $ for $\delta \le -1$,
%     \item $h(\delta) = 1 $ for $\delta \ge 1$.
% \end{enumerate}

\begin{equation}\label{eq:empReward2}
\hat{\calR}_B^{\epsilon}(\theta) =  \frac{1}{B}\sum_{j=1}^B  \sum_{n=0}^N D_{0,t_n}   P^{\theta,\epsilon,j}_{n}\ \varphi(X_{t_n}^j),
\end{equation}
and $\eqref{eq:empReward1}$ is recovered by letting $\epsilon  \downarrow 0$. 
The training phase is summarized in \cref{alg:cap}. To compute the initial value of the optimal stopping problem, we choose a large number of simulations $J\in \N$ and use the sharp boundary formulation $\eqref{eq:empReward1}$, i.e.   
$ \hat{v}_0 = \hat{\calR}_J(\theta^{(M)}) .$ 
%If one is interest in the whole  value function, %—that is, the price of the American option over time—
 %we simply set $\hat{v}(t,s) := \E^{\Q}[D_{t,\tau^{\theta^*}}\, \varphi(S^{t,s}_{\tau^{\theta^*}})]$ where $\theta^*$ is the "trained" parameter from $\eqref{eq:paramValue}$. 
 \begin{remark}
 Note that the value function is also  available at any intermediate time $t\in \calT_{\!N}$ once the threshold function has been trained. Indeed, one  can set 
 $\hat{v}(t,s) = \frac{1}{J}\sum_{j=1}^J   D_{t,\tau_t^{\theta,j}}  \ \varphi(X_{\tau_t^{\theta,j}}^j),$ 
 with $$\tau_t^{\theta,j} = \inf\{u \in \calT_{N} \cap \, [t,T]\ | \ \alpha(X_u) \ge G(u,\Xi(X_u); \theta) \} \wedge T.$$
 \end{remark}
 
%  \begin{equation}\label{eq:stopTime}
%     \tau_t^{\theta,j} = \inf\{u \in \calT_t \ | \ \alpha(X_u) \ge G(u,\Xi(X_u); \theta) \} \wedge T.
% \end{equation}


%   \begin{remark}
%  If one is interest in the whole  value function, %—that is, the price of the American option over time—
%  we can simply set $v^p(t,s) := \E^{\Q}[D_{t,\tau^{\theta^*}}\, \varphi(S^{t,s}_{\tau^{\theta^*}})]$ where $\theta^*$ is the "trained" parameter from $\eqref{eq:paramValue}$. 
%  \end{remark}
 
\subsection{Importance Sampling}
%As the training of the treshold function is made by exploring the reward obtained when hitting the boundary 

%As the training phase relies on the reward obtained when hitting the boundary, it is crucial that the simulated paths explore the state space frequently enough. 

%\label{eq:empReward2}

% Note that 

% As the training phase relies on the reward obtained when stopping, 

%Otherwise, the threshold function may not be trained properly. 
To properly train the threshold function in \cref{alg:cap}, 
%In light of \cref{alg:cap},
it is  crucial  that the simulated paths $\alpha(X^{j})$ hit the (fuzzy) boundary frequently enough and  across exercise dates. 
% Indeed, the reward 
% As the empirical reward $\eqref{eq:empReward2}$ , Moreover,  and across the exercise dates. Otherwise, 
This can be achieved using importance sampling, as explained below.  %technique, as explained below. %by changing the drift of the stock prices adequately, as explained below. 

Let $W$ be a $d-$dimensional Brownian motion under $\Q$ and suppose that $S$ evolves according to the stochastic differential equation, 
% \begin{align*}
%     dS_t = \mu(t,S_t) dt + \sigma(t,S_t) dW_t, \quad S_0 \in \R,
% \end{align*}
\begin{align}\label{eq:stockSDE}
    dS_t = \mu_t dt + \sigma_t dW_t, \quad S_0 \in \R,
\end{align}
where $\mu$ (resp. $\sigma$) is a  $d-$dimensional  (resp. $\R^{d\times d}-$valued) process that may depend on $t,S_t$ and other exogenous factors. 
Then consider the Girsanov transformation 
\begin{equation}\label{eq:Girsanov}
\frac{d\Q^{\lambda}}{d\Q} = \calE_T(- \lambda \bullet W), \quad \lambda  \text{ predictable},\quad   \E^{\Q}\left[\exp\left(\frac{1}{2}\lVert \lambda\rVert^2_{L^2([0,T];\R^d)}\right)\right] <\infty, 
\end{equation}%\E^{\Q}\left[e^{\frac{1}{2}\int_0^T \lVert \lambda_t\rVert^2 dt}\right] <\infty,  $$
so that 
$W_t^{\lambda} :=  \int_0^{t}\lambda_s ds + W_t$ is Brownian motion under $\Q^{\lambda}$. Therefore, $S$ has $\Q^{\lambda}-$dynamics
\begin{equation}
    dS_t = \mu^{\lambda}_t dt + \sigma_t dW^{\lambda}_t,\quad \mu^{\lambda}_t = \mu_t - \sigma_t \lambda_t. 
\end{equation}
If we define $Z^{\lambda}_t = \frac{d\Q}{d\Q^{\lambda}} \big |_{\calF_t}  = \calE_t(\lambda \bullet W^{\lambda})$, then the expected reward associated to some $\tau \in \vartheta(\calT)$ reads  $\E^{\Q}[D_{0,\tau}\, \varphi(X_{\tau})] =  \E^{\Q^{\lambda}}[Z^{\lambda}_{\tau}D_{0,\tau}\, \varphi(X_{\tau})]. $ %In turn, 
% \begin{equation*}\label{eq:OS}
%  \E^{\Q}[D_{0,\tau}\, \varphi(X_{\tau})] =  \E^{\Q^{\lambda}}[Z^{\lambda}_{\tau}D_{0,\tau}\, \varphi(X_{\tau})]. 
% \end{equation*}
%This produces only a minor change in 
In turn, 
the empirical reward $\eqref{eq:empReward2}$ becomes 
\begin{equation}\label{eq:empReward3}
\hat{\calR}_B^{\epsilon,\lambda}(\theta) =  \frac{1}{B}\sum_{j=1}^B  \sum_{n=0}^N Z^{\lambda,j}_{t_n} D_{0,t_n}   P^{\theta,\epsilon,j}_{n}\ \varphi(X_{t_n}^j).
\end{equation}
The process $\lambda$ can be chosen so as to increase the probability of crossing the boundary before maturity. In the numerical experiments, the process $\lambda$ is assumed to be constant for simplicity; see for instance  \cref{sec:putBS,sec:maxCallSym}. Therefore, $\lambda$ is in this case seen as a hyperparameter vector. %at some specific date %$t\in \calT$ 
%or 
%within a given time interval. We illustrate the use of importance sampling with a one-dimensional example.   %Also, we may wish to ensure that the 
%in some time interval $B \in \calB([0,T])$, i.e. $\Q^{\lambda}(\tau \in B)$ % or simply before maturity. 

% \begin{example}  Let $d=1$ and consider a standard call option  in the Black-Scholes model as in \cref{sec:1DExample}, i.e.  $\varphi(s)=(s-K)^{+}$ and choose  $\mu_t = (r-\delta)S_t$,  $\sigma_t = \sigma S_t$ in $\eqref{eq:stockSDE}$ . We recall from  \cref{ex:1DExample2} that  
% the threshold function does not depend on $S$ and the associated stopping time is   $\tau^{\theta} = \inf\{t\in [0,T] \ | \ S_t \ge G(t;\theta)\}$.

% We here show how importance sampling permits to control the probability of hitting the boundary strictly before maturity. That is, for a target probability $q\in [0,1]$, we would like to find $\lambda \in \R$ such that 
% \begin{equation}\label{eq:target}
% \Q^{\lambda}(\tau^{\theta} \leq T) = q, 
% \end{equation}
% with $\Q^{\lambda}$ given in $\eqref{eq:Girsanov}$. Of course, this cannot be guaranteed for all parameter vector $\theta$ simultaneously, so we instead require that $\eqref{eq:target}$ holds approximately. To this end, we employ an initial guess from the two parameter family given in $\eqref{eq:twoParams}$. Assuming that $\delta \ge r$, then we choose 
%  \begin{equation*}
%      f_0(t) = K\left(1  + \theta_1  \left(\frac{t}{T} \right)^{\theta_2} \right), \quad \theta = 
%      (\theta_1,\theta_2) \in \Theta_2 = \R^2_+.
%  \end{equation*}
 
 
% $X^{\lambda}_t = e^{Y^\lambda_t}$, with the log price given by $$Y^{\lambda}_t = \log x_0 +  (r-\sigma^2/2 - \lambda\sigma ) t  + \sigma W^\lambda_t. \qquad (W^\lambda: \, \Q^\lambda-\text{Brownian motion})$$


% Assume the initial exercise boundary to be of the form 
% $$f(t) = f_0^{\,1-t/T}\,K^{t/T} \; \Longrightarrow \; \log f(t) = \log f_0 + \frac{t}{T} \log \frac{K}{f_0}, $$
% so that $f(0)=f_0<x_0$ and $f(T)=K$. If $q \in [0,1]$ denotes the target probability of crossing the boundary before $T$, find $\lambda = \lambda(x_0)$ s.t.
% $\Q^{\lambda}(\tau \leq T) = q,$
% with $$ \tau = \inf\{t\in [0,T] \,|\, X_t^\lambda \leq f(t)\}. \quad (\inf\,  \emptyset = \infty)$$ Observe that 
% $$
% X_t^\lambda \leq  f(t) \; \Longleftrightarrow \;  Y_t^\lambda \leq \log f(t)
% \; \Longleftrightarrow \;  W^\lambda_t + \tilde{\xi} t \leq \tilde{\alpha},
% $$
% where \vspace{-3mm}
% \begin{align*}
%     \tilde{\xi}&=\xi/\sigma, \quad \xi = r-\sigma^2/2 - \lambda \sigma - \frac{1}{ T} \log \frac{K}{f_0},\\\vspace{-3mm}
%     \tilde{\alpha}&=\alpha/\sigma, \quad \alpha =  \log \frac{f_0}{x_0} <0.
% \end{align*}
%  Girsanov implies that $W^ {\tilde{\xi}} := (W^\lambda_t +  \tilde{\xi} t)_{t\in [0,T]}$ is a standard BM under
% $$\Q^{\tilde{\xi}} (d\omega) = \mathring{Z}_T \Q^\lambda(d\omega), \quad  \mathring{Z}_{\cdot} = \calE_{\cdot}(-\tilde{\xi} \bullet W^\lambda).$$
% Therefore, using the distribution of passage times for BMs with drift (\citealp{KaratzasShreve}, section 3.5 C),
% \begin{align*}
%     \Q^{\lambda}(\tau \leq T) &= \E^{\Q^{\tilde{\xi}} }[\mathring{Z}^{-1}_T\, \mathds{1}_{\{\tau \leq T\}}]
%     = \E^{\Q^{\tilde{\xi}} }[\mathring{Z}^{-1}_{\tau }\, \mathds{1}_{\{\tau \leq T\}}]
%     = \int_0^T \frac{|\alpha|}{\sqrt{2 \pi t^3\sigma^2}}\exp\left\{- \frac{(\alpha - \xi t)^2}{2 t \sigma^2}\right\} dt.
% \end{align*}
% %We can solve numerically for $\lambda$ (located in $\xi$)  by setting the above expression equal to our target $q$. Figure \ref{fig:Girsanov} displays the function $x_0 \mapsto \lambda(x_0)\cdot \sigma$ for $q \in \{0.75,0.90\}$.
% \end{example}



%\subsubsection{Regularity}
%\input{FB/Section 1/Regularity}


%\subsection{Convergence}