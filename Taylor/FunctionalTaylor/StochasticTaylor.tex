\subsection{Functional Itô Calculus}
  The concepts of spatial and temporal derivative for functionals (\citet{Dupire}) are now recalled. 

\begin{definition}
Let $X \in \Lambda$ and $f:\Lambda \to \R$ be a functional. Assuming existence of the following quantities, we define
\begin{itemize}
    \item The \textit{spatial derivative},
    $$\Delta_x f(X_t) = \lim_{\delta x\to 0} \, \frac{f(X^{\delta x}_t)-f(X_t)}{\delta x}, $$
    with the bumped path $X^{\delta x}_t(s) =  x_s + \delta x\,\mathds{1}_{\{s\,=\,t\}}$, $s\in [0,t]$.
     \item The \textit{temporal derivative},
    $$\Delta_t f(X_t) = \lim_{\delta t\downarrow 0} \, \frac{f(X_{t,\delta t})-f(X_t)}{\delta t},$$
    with the flat extension $X_{t,\delta t}(s) = x_{s \wedge t}$, $s\in [0,t+\delta t ]$. %$X_{t,\delta t}|_{[0,t]} = X_{t}$, $X_{t,\delta t}|_{[t,t+\delta t]} \equiv x_{t}$.
\end{itemize}
\end{definition}
Multiple derivatives are denoted by  $\Delta_{\alpha} = \Delta_{\alpha_1} \cdots \Delta_{\alpha_k}$. For instance, 
$\Delta_{101}f = \Delta_{x} (\Delta_t(\Delta_x f))$, where we stress again that  $0,1$ is identified with $t,x$, respectively. Besides if $\alpha = \emptyset$, then $\Delta_{\alpha}$ is the identity operator.

A crucial remark is that $\Delta_x$ and $\Delta_t$ do not commute. Put another way, the \textit{Lie bracket},
$[\Delta_x,\Delta_t]f := \Delta_{xt}f- \Delta_{tx}f$ is in general non-trivial. See Example \ref{ex:QV}. The Lie bracket provides an instantaneous path-dependence gauge, as explained in  \cite{Jazaerli}. 
%In particular, a functional $f$ that satisfies $[\Delta_x,\Delta_t]f=0$ is called \textit{locally weakly path-dependent}.  

\begin{example}\label{ex:IntDev}
We compute the functional derivatives of the time integral $f(X_t) = \int_0^t \varphi(X_s) ds$.
Since $\{ s\in [0,t] \, | \, X_t(s) \ne X^{\delta x}_t(s)\} = \{ t \}$ has Lebesgue measure zero, we conclude $\Delta_xf \equiv 0$. For the time derivative, we simply obtain  
    \begin{align*}
        \Delta_t f(X_t) &= \lim_{\delta t\downarrow 0} \,  \frac{1}{\delta t}\int_t^{t+\delta t} \underbrace{X_{t,\delta t}(s)}_{= \, x_t}  ds = x_t.
    \end{align*}
\end{example}

\begin{example}\label{ex:QV} Consider the quadratic variation functional, namely
$$\langle X \rangle_t =  \lim_{N \to \infty} \sum_{t_i\, \in \, \Pi_{N}} (x_{t_i}-x_{t_{i-1}})^2,$$
where $(\Pi_N) = (\{0 = t_0 < \ldots < t_N = t\})$ is a sequence of refining partitions of $[0,t]$.
First, the time derivative of $f$ is nil as a flat-extension  does not provide additional variation. On the other hand, the space derivative reads
\begin{align*}
    \Delta_x \langle X \rangle_t &= \lim_{\delta x \to 0} \frac{\langle X^{\delta x} \rangle_t-\langle X \rangle_t}{\delta x} = \lim_{\delta x \to 0} \lim_{N \to \,\infty} \frac{(x_t + \delta x - x_{t_{N-1}})^2 - (x_t - x_{t_{N-1}})^2}{\delta x} = 2(x_t-x_{t-}).
\end{align*}
Indeed, bumping a path alters only  the last (infinitesimal)   increment. %its last increment along any partition.
% Theorem \ref{thm:FSF} yields the representation
% $$\langle X \rangle_t =  \int_0^t 2\,(x_s-x_{s-}) \circ dx_s.$$
Looking at the mixed derivatives, we have respectively $\Delta_{xt}\langle X \rangle_t = 0 $ and 
$$\Delta_{tx}\langle X \rangle_t = -2\lim_{\delta t \downarrow 0} \frac{x_{t} - x_{t-}}{\delta t} =   \begin{cases} \infty, & x_{t} - x_{t-} < 0, \\
- \infty, & x_{t} - x_{t-} > 0,\\
 0, & x_{t} = x_{t-}.
 \end{cases}
$$
%- \text{sign}(x_{t} - x_{t-}) \infty,$$
%where we assume that $\text{sign}(0)=0$. 
Therefore, the Lie bracket of $f$ differs from $0$ when $X$ jumps at $t$.
% \bb{From the relationship between It\^o and Stratonovich integrals, we obtain alternatively
% $$\langle X \rangle_t = 2 \left[ \int_0^t x_s\circ dx_s - \int_0^t  x_s dx_s \right], $$ 
% from which we must have 
% $$  \int_0^t x_{s-} \circ dx_s = \int_0^t  x_s dx_s \; \Longrightarrow \langle X_{-}, X \rangle = 0 \qquad (?)$$}
\end{example}



As is customary,  regularity conditions are required on the function to expand. More specifically, we consider the classes 
$$\C^{K,L} = \{f:\Lambda \to \R \, |\, \Delta_{\alpha}f \text{ exists and is $\Lambda-$continuous } \, \forall \, 
\alpha \text{ s.t. }\, |\alpha|_0 \le K, \,  |\alpha|_1 \le L\}, \quad K,L \ge 0.$$ 

We adapt the Functional It\^o formula  \cite{Dupire} to Stratonovich integrals. %; see \cite{LittererOberhauser} for a proof. 

\begin{theorem}\textnormal{(\textbf{Functional Stratonovich formula})}\label{thm:FSF} Let $X$ be a semimartingale. Then for any  $f\in \C^{1,1}$, 
\begin{align*}
    f(X_t) &= f(X_0) + \int_0^t \Delta_t f(X_s)  ds + \int_0^t\Delta_x f(X_s) \circ dx_s. 
\end{align*}
%If the trajectories have finite variation $\Q-a.s.$, the last term  boils down to a Riemann-Stieltjes integral. 
\end{theorem}
\begin{proof}
This is an immediate consequence of the Functional It\^o formula  \cite{Dupire}.
\end{proof}


\begin{example}\label{ex:StratDev}
We  repeat the same exercise as in \cref{ex:IntDev} for a semimartingale $X$ and the Statonovich integral $f(X_t) = \int_0^t \varphi(X_s) \circ dx_s$. Since $f(X_0)=0$,  \cref{thm:FSF} gives
$$\int_0^t \varphi(X_s) \circ dx_s =  \int_0^t \Delta_t f(X_s) ds + \int_0^t \Delta_x f(X_s) \circ dx_s.  $$
We thus conclude that $\Delta_t f =0$ and  $\Delta_x f =\varphi$. 
This rather expected result holds as the Stratonovich integral gives a first order calculus. 
However, if $\Q$ is the Wiener and we consider the It\^o integral $f(X_t) = \int_0^t \varphi(X_s)  dx_s$ instead, then the temporal derivative may be non-zero. Indeed, taking $\varphi(X_t)=x_t$ for concreteness, the functional It\^o formula gives 
$$\frac{x_t^2}{2} - \frac{t}{2}  = \int_0^t x_s dx_s =  \int_0^t (\Delta_t + \frac{1}{2}\Delta_{xx}) f(X_s) ds + \int_0^t \Delta_x f(X_s) dx_s,  $$
hence $\Delta_x f(X_t)= x_t$  and  $\Delta_t f(X_t) = -\frac{1}{2} \Delta_{xx} f(X_t) = -\frac{1}{2}$. 
% with $\alpha \in \{0,1\}$. 
%First, note that the integrand $\varphi(X_s)$ is understood as $\varphi(X_{s-})$ when $X$ is not left continuous.  As the Stratonovich integral generates a first order calculus, we necessarily have 
%$$f(X_t) =  f(X_0) + \int{0}^t \Delta_x $$
    % \begin{equation*}% \label{eq:prop}
    %     \Delta_x f(X_t) = \frac{\varphi(X_{t-})  +\varphi(X_{t}) }{2} + \frac{x_t - x_{t-}}{2} \Delta_x \varphi(X_{t}). 
    %  \end{equation*}

% \rr{First, the horizontal derivative is clearly zero. On the other hand,  
%   it is easily verified from the construction of the Stratotonich integral that
%     \begin{equation*}% \label{eq:prop}
%         \Delta_x f(X_t) = \frac{\varphi(X_{t-})  +\varphi(X_{t}) }{2} + \frac{x_t - x_{t-}}{2} \Delta_x \varphi(X_{t}). 
%     \end{equation*}
% %the vertical derivative is given by
%   When $X$ is  continuous at $t$, the spatial derivative reads $\Delta_x f(X_t)=\varphi(X_{t})$. }
\end{example}


\subsection{Main Results}

We first show how to expand a functional around the initial value of the path. Borrowing the terminology from differential calculus, we call such a series a  \textit{Functional  Maclaurin expansion}.  

\begin{theorem}\textnormal{(\textbf{Functional  Maclaurin expansion})} \label{thm:FME}
Let $f$ be a functional in $\C^{K,K}$. Then for any $t\in [0,T]$, the following expansion holds,
\begin{align}\label{eq:FSTE}
    f(X_t) = \sum_{|\alpha|< K}  \Delta_{\alpha}f(X_0) \calS_{\alpha}(X_t) + r_{K}(X_t),
\end{align}
with the  remainder functional
\begin{equation}\label{eq:remainder}
    r_{K}(X_t) = \sum_{|\alpha| = K} \int_{\triangle_{K,t}} \Delta_{\alpha}f(X_{t_1}) \circ \, dx^{\alpha}.
\end{equation}
\end{theorem}

\begin{proof}
See \cref{app:FME}.
\end{proof}

The functional Taylor expansion presents obvious benefits to approximate functionals.
 Indeed, a projection for $f \in \C^{K,K}$ is obtained by cutting off the functional series, namely%\footnote{Contrary to  \Cref{thm:FSF},  the initial $k-$th order derivatives are added as the remainder$-$necessitating $f\in \C^{K+1}-$is  ignored.}  
\begin{equation*}
     f^{K,\calS}(X_t) := \sum_{|\alpha|\, \le \, K} \Delta_{\alpha}f (X_0) \calS_{\alpha}(X_t).
\end{equation*}
    In other words, $ f^{K,\calS}(X)$ approximates the transformed path $ f(X)$ by a linear combination of signature elements. %In quantitative finance, %the context of option pricing, 
    The latter is referred to  as a \textit{polynomial functional}  \cite{LittererOberhauser} or \textit{signature payoff} \cite{LyonsNum}. %, often written as 
    % $$\langle \xi, \calS(X_t) \rangle =  \sum_{\alpha } \xi_{\alpha} \calS_{\alpha}(X_t), $$ %\varphi_{\xi,K}(X_t) 
%for some deterministic coefficients $(\xi_{\alpha})$. 
As shown in \cite{LittererOberhauser} using the Stone–Weierstrass theorem, the class of signature payoffs is dense in the space of functionals  when restricted to paths of bounded variation.  
%In other words, the signature elements play the same role as the polynomials in the Stone–Weierstrass theorem. 
However, this only guarantees the existence of a signature payoff arbitrarily close to a functional. The functional Maclaurin  series, on the other hand, makes the approximation explicit.

% It is a priori unclear whether the functional Taylor expansion provides an optimal procedure, in some suitable sense, to approximate functionals. Indeed, alternative methods exists, based notably on the Wiener-Itô chaos expansion (see \Cref{sec:chaos}) or the Karhunen-Loève transform \cite{Tissot}. A %thorough 
% performance comparison of these approaches goes, however, beyond the scope of this work. 
We now relax \Cref{thm:FTE} by allowing expansions around a piece of the path. In fact, the expansion can be also done backward in time  as we shall see below. 
Let $\oplus: \Lambda^2 \to \Lambda$ be the noncommutative binary operation that concatenate paths in a continuous fashion. In other words,   if $X\in \Lambda_{s}$, $ Y \in \Lambda_{u}$ then $W = X \oplus Y \in \Lambda_{s+u}$ is given by 
$$ w_r  = x_{r \wedge s} + y_{(r- s)^+}- y_0, \quad  r \in [0,s+u].$$
If the horizon $T$ is finite and $s+u > T$, we define instead $X \oplus Y = X \oplus Y_{T-s} \in \Lambda_T$. %Note that the \textit{null path} $\boldsymbol{0} \in \Lambda_0$ the neural element associated to $\oplus$.
Moreover, if  $X\in \Lambda_T$ and $ s \le t $, then $X_s = X |_{[s,t]} \in \Lambda_{t-s}$ is  the restriction of $X$  to $[s,t]$, i.e.  $X |_{[s,t]}(u) = x_{s+u}$, $u \in [0,t-s]$. %This implies that if $X$ has independent increments, then  $X_s$ and $X |_{[s,t]}$ are independent. 
Also, we  can define $X |_{[t,s]}$ as $\overleftarrow{X |_{[s,t]}}$,  the time-reversed version of $X |_{[s,t]}$, 
$$X |_{[t,s]}(u) = \overleftarrow{X |_{[s,t]}}(u)  =  x_{t-u} , \quad  u \in [0,t-s]. $$
  %Note also that  $X |_{[s,t]}=\overleftarrow{X}|_{[T-t,T-s]}$. 
We adopt the convention that $X_t \oplus X|_{[t,s]} = X_s$ so concatenating a time-reversed path makes the resulting path shorter. %reduces the length. 
Let us  now revisit  \Cref{thm:FME}. 
%Moreover, let $X^{-1}$ be the right inverse of $X$ with respect to $\oplus$, namely $X \oplus X^{-1} = \boldsymbol{0}$. 
%$$Y = X_t\ominus X_s \ \Longleftrightarrow \   X_s\oplus Y = X_t, \quad t,s \in [0,T].  $$

%For  $0\le s\le t \le T$,  define the section let $X|_{[s,t]} \in \Lambda_{t-s}$ denote the path $X$ restricted to  the interval $[s,t]$, i.e. $X_{s,t}(u)=x_{u+s}$, $u\in [0,t-s]$.  
% \begin{corollary}
%  Let $f$ be a functional in $\C^{K,K}$. Then for $0\le s\le t \le T$, the following expansion holds,
% \begin{align}\label{eq:FSTE}
%     f(X_t) = \sum_{|\alpha|< K}  \Delta_{\alpha}f(X_s) \calS_{\alpha}(X_{s,t}) + r_{K}(X_{s,t}),
% \end{align}
% with  $
%     r_{K}(Y) = \sum_{|\alpha| = K} \int_{\triangle_{K,t-s}} \Delta_{\alpha}f(Y_{t_1}) \circ \, dy^{\alpha}.
% $, $Y\in \Lambda_{t-s}$. 
% \end{corollary}

\begin{theorem}\textnormal{(\textbf{Functional  Taylor expansion})}
\label{thm:FTE}
 Let $f\in \C^{K,K}$ and $ s, t \in [0,T]$. Moreover, write ,  $Y_{u} = X |_{[s,t]} \in \Lambda_u$ with $u=|t-s|$. Then, 
\begin{align}
    f(X_{t}) &= \sum_{|\alpha|< K}  \Delta_{\alpha}f(X_s) \calS_{\alpha}(Y_u) + \tilde{r}_{K}(X_s,Y_u), \label{eq:CorFTE1}\\
        \tilde{r}_{K}(X_s,Y_u) &= \sum_{|\alpha| = K} \int_{\triangle_{K,u}} \Delta_{\alpha}f(X_{s}\oplus Y_{t_1}) \circ \, dy^{\alpha}. \label{eq:CorFTE2}
\end{align}
where we  identify $X_{s}\oplus Y_{t_1}$ with $X_{s-t_1}$ when $s>t$ and $X_{s+t_1}$ otherwise.
\end{theorem}

\begin{proof}
See \cref{app:FTE}.
\end{proof}

Formula $\eqref{eq:CorFTE1}$ can be used to price (or hedge) forward-starting options when $t\ge s$; \bb{see Example ...}
When $t< s$, we obtain an \textit{anticipative functional series}. 
We now establish an estimate of the remainder when the derivatives of the associated functional are uniformly bounded. 

\begin{proposition}
Assume that $X$ is continuous and  $f \in \C^{K,K}$ with $\Delta_{\alpha}f \in L^{1}(\Lambda)$. Then there exists a constant $C$ depending on $K$ and $\Q$ such that the remainder given in \Cref{thm:FTE} satisfies %, i.e. $\lVert \Delta_{\alpha}f \rVert_{L^{1}(\Lambda)}:= \sup_{0\le t \le T} \lVert \Delta_{\alpha}f \rVert_{L^{1}(\Lambda_t)} < \infty$. Then 
$$\lVert r_K \rVert_{L^{1}(\Lambda)} \le C \frac{2^K}{K!} \max_{|\alpha|=K}\,  \lVert \Delta_{\alpha}f \rVert_{L^{1}(\Lambda)}.$$

\begin{proof}
Let $t\in [0,T]$. First, the definition of $r_K$ gives
$$\E^{\Q}[|r_K(X_t)|] \le \max_{|\alpha|=K}\,  \lVert \Delta_{\alpha}f \rVert_{L^{1}(\Lambda)} \sum_{|\alpha|=K} \E^{\Q}[|\calS_{\alpha}(X_t)|].$$ Second, for continuous paths, we have  $\sup_{0\le t \le T}\E^{\Q}[|\calS_{\alpha}(X_t)|] \leq \frac{C_{\alpha}}{K!}$ for some $C_{\alpha}$ depending only on $\alpha$ and $\Q$. The above bound follows immediately from the  $\Q-a.s.$ boundedness of $X$ on the compact interval $[0,T]$ and the fact that the simplex $\triangle_{K,t}$ represents a fraction  $\frac{1}{K!}$ of the hypercube $[0,t]^K$. Setting $C= \max_{|\alpha|=K} \,C_{\alpha}$ finishes the proof.

\end{proof}
\end{proposition}
 

 


\subsection{Examples}
 We now illustrate \Cref{thm:FTE,thm:FME} with several examples. 
\begin{example}
%     
%Since $\{ s\in [0,t] \, | \, X_t(s) \ne X^h_t(s)\} = \{ t \}$ has Lebesgue measure zero, we conclude $\Delta_xf \equiv 0$. The only non-zero derivatives are  
%     \begin{align*}
%         \Delta_t f(X_t) &= \lim_{\delta t\downarrow 0} \,  \frac{1}{\delta t}\int_t^{t+\delta t} \underbrace{X_{t,\delta t}(s)}_{= \, x_t}  ds = x_t, \\
%     \Delta_{xt} f(X_t) &= \Delta_x (\Delta_t f(X_t)) = \Delta_x x_t = 1.
%     \end{align*}
  Let $f(X_t) = \int_0^t x_s ds$. Thanks to \Cref{ex:IntDev}, we obtain $\Delta_x f =0$, $\Delta_t f = x_t$. The only nonzero derivative of higher order is therefore $\Delta_{xt}f=1$.
 \Cref{thm:FME} gives the trivial expansion
    $$f(X_t) = f(X_0) \calS_{\emptyset} + \Delta_t f(X_0) \calS_{0} + \Delta_{xt} f(X_0) \calS_{10} = x_0 \, t + \int_0^t (x_s - x_0)ds. $$
    This implies that Asian forward contracts$-$entailing the integrated price $f(X_T)$ in a linear fashion$-$can be statically replicated by signature elements of order at most $2$.
\end{example}

\begin{example}
Let $h:\R\to \R$ be an analytic European payoff.
%consider  an analytic European payoff function $h\in \calC^{w}(\R)$. 
Going back to the fundamentals, Taylor's theorem gives %grounding of calculus
\begin{equation}\label{eq:stdTaylor}
    h(x_t)= \sum_{k=0}^{K-1} h^{(k)}(x_0) \frac{(x_t-x_0)^k}{k!} + r_K(x_t), \quad r_K(x_t)= o(|x_t-x_0|^{K-1}).
\end{equation}
 It is often overlooked that the scaled monomials appearing in the expansion are in fact iterated integrals. Indeed, 
$$\frac{(x_t-x_0)^k}{k!} = \int_0^t \cdots \int_0^{t_2} \circ \, dx_{t_1}\ldots \circ dx_{t_k} = \calS_{\mathds{1}_k}(X_t). $$ 
Moreover, the integral form of the remainder allows us to write
\begin{align*}
    r_K(x_t) = \int_{x_0}^{x_t} h^{(K)}(z) \frac{(z-x_0)^{K-1}}{(K-1)!} dz 
    = \int_{\triangle_{K,t}} h^{(K)}(x_{t_1}) \circ \, dx^{\mathds{1}_k}.
\end{align*}

%On the other hand, one can define the functional
On the other hand, we can embed $h$ into the space of functionals by defining
$f(X_t) = h(x_t)$. Of course, $\Delta_x f = \partial_x h$ and $\Delta_t f = \partial_t h = 0$. Hence $f\in \C^{K,K}$ and  \Cref{thm:FME} yields
\begin{align*}
    f(X_t)  %\sum_{|\alpha|< K} \Delta_{\alpha} f(X_0)\calS_{\alpha}(X_t) + r_K(X_t)\\
    &= \sum_{k=0}^{K-1} \Delta_{\mathds{1}_k} f(X_0)\calS_{\mathds{1}_k}(X_t) +  \int_{\triangle_{K,t}} \Delta_{\mathds{1}_k} f(X_{t_1}) \circ \, dx^{\mathds{1}_k},
\end{align*}
which is consistent with $\eqref{eq:stdTaylor}$. We conclude that the elements $\{\calS_{\emptyset}, \calS_{1},\calS_{11},\calS_{111}, \ldots\}-$the rightmost diagonal of the signature tree; see $\eqref{eq:tree}-$suffices to replicate European payoffs. Equivalently, they allow to  recover the volatility surface. 

\end{example} 
%\bb{Stone Weierstrass (calibration) vs explicit (here) and incremental construction. }
    %Observe that the image $\pi^{K,\calS}(\C^K) $ lies in the vector space of \textit{Stratonovich polynomials} of degree less than $K$,
    %$$\calP^{\circ}_K = \left \{\sum_{|\alpha|_0 \, \le \, K} \xi_{\alpha} \calS_{\alpha}, \quad \xi \in \R^{2^{K+1}-1} \right\}, $$
    %also referred to as \textit{signature payoffs} in the context of option pricing. 
    
%     \begin{definition}
% A functional $f$ is a \textbf{signature payoff}  if it is a  linear  function of the signature, i.e.
% $$f(X_t) = \langle \xi ,\calS(X_t) \rangle  =  \sum_{\alpha } \xi_{\alpha} \calS_{\alpha}(X_t), $$
% for some deterministic coefficients $\xi \in \calT((\R^2)) := \bigoplus_{k=0}^\infty (\R^2)^{\otimes k}$.
% %In other words, it is a linear combination of entries of the signature process of $X$ with coefficients $a \in \R^{\N}$.
% \end{definition}


\begin{example} \label{ex:classicTaylor}
We verify that \Cref{thm:FTE} is a generalization of %consistent with 
the classical Stratonovich-Taylor expansion \cite[Theorem~5.6.1.]{KP}. For simplicity, we stick to the one-dimensional case. Let $X$ be Brownian motion and consider the Stratonovich stochastic differential equation, 
\begin{equation}\label{eq:SDE}
    dy_t = a(t,y_t)dt + b(t,y_t) \circ dx_t, \quad y_0 \in \R,
\end{equation}
for smooth (hence Lipschitz) measurable functions $a,b: [0,T]\times \R \to \R$. If we further assume that $a,b$ have   %Lipschitz continuous in $y$, uniformly in time, and of 
 at most linear growth, there exists a unique strong solution of $\eqref{eq:SDE}$ \cite[Theorem~4.5.3.]{KP}.
% Moreover, the adaptedness of $y$ w.r.t. the filtration of $X$ ensures the existence of a functional $\psi:\Lambda_T\to \R$, often called the \textit{Itô map}, such that 
% \begin{equation}\label{eq:SDEFunc}
%     \psi(X_t) = y_t = y_0 + \int_0^t a(s,y_s)ds +  \int_0^t b(s,y_s) \circ dx_s.
% \end{equation}
 Moreover, the adaptedness of $y$ %w.r.t. the filtration of $X$ 
 ensures the existence of a functional $\calI:\Lambda\to \R$, often called the \textit{Itô map}, such that 
% \begin{equation}\label{eq:SDEFunc}
%     \calI(X_t) = y_t = y_0 + \int_0^t a(s,y_s)ds +  \int_0^t b(s,y_s) \circ dx_s.
% \end{equation}
\begin{equation}\label{eq:SDEFunc}
    y_t = \calI(X_t)  = y_0 + \int_0^t a(s,\calI(X_s))ds +  \int_0^t b(s,\calI(X_s)) \circ dx_s.
\end{equation}
Given $h \in \calC^{K,K}([0,T],\R)$ and $s\le t$, the Stratonovich-Taylor expansion  reads
\begin{align}\label{eq:STE}
    h(t,y_t) &= \sum_{|\alpha|< K}  \calL_{\alpha}h(s,y_s) \calS_{\alpha}(X_t) + \sum_{|\alpha| = K} \int_{\triangle_{K,t-s}} \calL_{\alpha}h(s+t_1,y_{s+t_1})\circ \, dx^{\alpha},
\end{align}
with the differential operators $\calL_0 = \partial_t + a \,\partial_y$, $\calL_1=b\, \partial_y$, and $\calL_{\alpha} =\calL_{\alpha_1}\ldots \calL_{\alpha_k}$. Now define the functional $f(X_t) = h(t,\calI(X_t))$. Comparing $\eqref{eq:STE}$ with  \Cref{thm:FTE}, we gather that  $\calL_{\alpha}h = \Delta_\alpha f$ must hold. Even though the regularity of $f$ is a priori not guaranteed, we will see that the smoothness of $h$, $a$, $b$ carries over to $f$. 

It remains to show that $\calL_\alpha$ acts on $h$ exactly as $\Delta_\alpha$ acts on $f$ for $\alpha=0,1$. Indeed, the same would be true for longer words due to the similar recursive structure of the differential operators. If $\alpha=0$, the chain rule, equation $\eqref{eq:SDEFunc}$ and Example \ref{ex:IntDev} give respectively 
% \begin{align*}
%     \Delta_{t}f(X_t) = \partial_t h(t,y_t) + \partial_y h(t,y_t) \, \Delta_t\psi(X_t) = (\partial_t  + a \, \partial_y)h(t,y_t)
% \end{align*}
\begin{align*}
    \Delta_{t}f = \partial_t h + \partial_y h \, \Delta_t\! \calI = (\partial_t  + a \, \partial_y)h = \calL_0 h.
\end{align*}
As $X$ is continuous $\Q-$a.s., then $\Delta_x\calI=b$ thanks to Example \ref{ex:StratDev}. We hence obtain %for $\alpha=1$, 
\begin{align*}
    \Delta_{x}f = \partial_y h \, \Delta_x \!\calI = b \, \partial_y h = \calL_1 h.
\end{align*}
In conclusion, the two expansions coincide as claimed.
\hfill $\square$

%$\calL$ and $\Delta$. 


\end{example}

% \begin{example} \label{ex:classicTaylor}
% Let $f(X_t) =  h(t,x_t) $ for an analytic  function $h$. As the functional is path-independent, we  immediately get $\Delta_t = \partial_t$, $\Delta_x = \partial_x$. Hence the differential operators commute, which gives
% % $$\Delta_{\alpha} f(X_t) = %\frac{\partial^k}{\partial x ^{|\alpha|} \partial t^{k - |\alpha|}}
% % \partial^{|\alpha|}_x \partial^{k - |\alpha|}_t
% % h(t,x_t),\quad  \alpha \in \{0,1\}^{k}, \quad |\alpha| = \sum_{j=1}^k \alpha_j\,.$$
% \begin{align*}
%     f(X_t) &= \sum_{l(\alpha)< K}  \partial^{|\alpha|}_x \partial^{l(\alpha) - |\alpha|}_t h(0,x_0) \calS_{\alpha}(X_t) + r_{K}(X_t), \quad |\alpha| = \sum_{k=1}^{l(\alpha)} \alpha_k,\\
%     r_{K}(X_t) &= \sum_{l(\alpha) = K} \int_{\triangle_{K,t}} \partial^{|\alpha|}_x \partial^{K - |\alpha|}_t h(t_1,x_{t_1}) \circ \, dx_{\alpha},
% \end{align*}
% We thus recover the  Stochastic Stratonovich-Taylor expansion (\citet[Theorem~5.6.1.]{KP}) where $X$ is Brownian motion.
% \end{example}

% \bb{\begin{example}
% Let $f(X_t) = \int_0^t h(x_s)ds $ for an analytic real function $h$. Since $\{ s\in [0,t] \, | \, X_t(s) \ne X^h_t(s)\} = \{ t \}$ has Lebesgue measure zero, we conclude $\Delta_xf = 0$. On the other hand, 
% \begin{align*}
%     \Delta_t f(X_t) = \lim_{\delta t\downarrow 0}  \frac{1}{\delta t}\int_t^{t+\delta t} \underbrace{h(X_{t,\delta t}(s))}_{h(x_t)}  ds = h(x_t),
% \end{align*} 
% from which we conclude
% $$
%     \Delta_{\alpha}f(X_t) = \begin{cases} h^{(k)}(x_t), & \alpha = \alpha^{(k)} ,  k\ge 0,\\
%     0, & \text{otherwise}.
%     \end{cases}, \quad \alpha^{(k)} := (1,\ldots,1,0) \in \{0,1\}^{k+1}.
% $$
% On the other hand, note that 
% $$\calS_{\alpha^{(k)}}(X_t) = \int_0^t \calS_{(1,\ldots,1)}(X_s)ds = \int_0^t \frac{(x_s-x_0)^k}{k!}ds. $$
% Thus, 
% $$ f(X_t) = \sum_{k=0}^{\infty} h^{(k)}(X_0) \int_0^t \frac{(x_s-x_0)^k}{k!}ds =    \int_0^t \sum_{k=0}^{\infty} h^{(k)}(X_0)\frac{(x_s-x_0)^k}{k!}ds, $$
% which is a classical Taylor expansion integrated over $[0,t]$... why not simply $f(X_t) = h(x_t)$?
% \end{example}}





% \subsubsection{Effect of (Non)Linear Path Transformation in the Functional Taylor Expansion}
% \begin{itemize}
%     \item $X^{\lambda} = \lambda X$
%     \item $X^{Z} = X \oplus Z$
% \end{itemize}
% Terminal functional and Stone-Weierstrass: 
% Incremental Construction, 
% $$(\pi^{K+1,\calS}-\pi^{K,\calS})g(X_T) = \sum_{l(\alpha) = K+1} \xi_{\alpha} \calS_{\alpha}(X_T),$$
% Replace $T$ by $t$ to obtain an extrapolated functional $f(X_t)$.
% Knowledge of $\xi_{\alpha} \Longrightarrow$ characterize $f$ uniquely? 
% \begin{example} Let $\Q$ be the Wiener measure and consider the Bachelier model 
% $s_t =v(X_t)=  rt + \sigma x_t,$ and the payoff  
% $$h(S_T)= \frac{1}{T}\int_0^T s_t dt -K  \quad (\text{Asian forward}).$$
% Then $h(S_T) = f(X_T)$, with $f = h\circ v$. (To be continued...)

% can be written as $h(S_T)=G(X_T)$, with
% \begin{align*}
% G(X_T) &= \frac{r}{T}x_{T,1} + \frac{\sigma}{T}\int_0^T x_{t,2} dt - K\\
% &= r-K + \frac{\sigma}{T}\int_0^T w_t dt \\
%     &= (r-K)J^\circ_0(X_T) + \frac{\sigma}{T} \,J^\circ_{2,(1,0)}(X_T)
% \end{align*}
% We conclude that $h(S_T) = \langle \varphi, \calS(X_T)\rangle$, where  $\varphi=(r-K,0,0,0,0,\frac{\sigma}{T},0,\ldots)$.
%\end{example}
    
% OR (Notation)

% \begin{align*}
%     f(X_t) &= f(x_0) + \int_0^T\Delta_t f(X_s) ds + \int_0^t\Delta_x f(X_s) \circ dx_s\\
% &=  f(x_0) \calS_{\emptyset}(X_t)+ \Delta_t f(x_0) \calS_{t}(X_t) + \Delta_x f(x_0) \calS_{x}(X_t)\\
% &+ \int_0^t\int_0^{t_1}\Delta_{tt} f(X_{t_1})  dt_1dt_2 + \int_0^t\int_0^{t_1}\Delta_{xt} f(X_{t_1})  dx_{t_1}dt_2 + \ldots \\
% & = \ldots + \calS_{ttxt}(X_t) \ldots \\
% &= \sum_{|\alpha|_0\le K}  \Delta_{\alpha}f(x_0) \calS_{\alpha}(X_t) + \epsilon_{K+1}.
% \end{align*} 


% \begin{example} 
% Time average: ...
% \end{example}

% \begin{example} (Total variance): $f(X_t) = \int_0^t x_s^2 ds$. The non-zero derivatives are  $\Delta_t f =x_t^2$, $\Delta_{xt}f =2x_t$, $\Delta_{xxt}f =2$, therefore
% $$f(X_t) = x_0^2 \calS_{(0)} + \, 2x_0 \calS_{(1,0)} + \, 2 \calS_{(1,1,0)}. $$
% The above identity can be also checked by a straightforward calculation.
% \end{example}

%====================================%
%====================================%
%====================================%
 %\subsubsection{Radius of Convergence}

% It can be defined as, 

% $$\frakR(f,x_0) = \sup\{\delta \ge 0 \, | \, \lim_{K\to \infty}\pi^{K,\calS}f(X') \text{ is well-defined } \forall X'\in B_{\delta}(x_0)  \},$$
% where, in general, $B_{\delta}(X_t)$ is the $\lVert \cdot \rVert_{\Lambda}$-ball of radius $\delta$ centered at $X_t\in \Lambda$, i.e.
% $$\forall X_s' \in B_{\delta}(X_t), \quad \delta \ge \lVert X_t-X_s' \rVert_{\Lambda} \quad \left(= \lVert X_t-X_{s,t-s}' \rVert_{\infty} + t-s \;\text{ if } \; s \le t\right).$$

% Notice that paths in $B_{\delta}(x_0)$  must lie in the triangle with nodes $\{(\delta,0), (0,\delta), (0,-\delta)\}$.

% \begin{figure}[H]
%     \centering
%     \includegraphics[scale =0.2]{Figures/DeltaBall.jpg}
% \end{figure}
