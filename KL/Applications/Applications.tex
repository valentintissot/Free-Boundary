\section{Applications} \label{sec:application}
We now illustrate the benefits of the Karhunen-Loève expansion on functionals for the pricing of exotic derivatives. % written on a single asset. 
We slightly change notations and write $W$ for the coordinate process. The path $X$ now represents the stock price where for simplicity, we employ the Black-Scholes model with zero interest rate. That is,  $\Q$ is the Wiener measure and $x_t = x_0 \calE_t(\sigma  W)$, where $x_0>0$ is fixed and  $\calE$ denotes the stochastic exponential.
Notice, however, that our method applies to any dynamics of the underlying.  

%$\varphi_m(X_t) = (f(X_t)-m\, x_0)^{+}, \, f: \Lambda \to \R,$
%where $m$ is the moneyness of the option. 
%In other words, we restrict ourselves to call options written on a path-dependent quantity of the underlying stock price.  
Let $\calM \subseteq \R$, $\calT \subseteq [0,T]$ be a finite set of option parameters and maturities, respectively. 
We seek to approximate the price surface
$p: \calM \times \calT \to \R$,  $p(m,\tau) = \E^{\Q}[ \varphi_m(X_\tau)]$, where the payoffs  $\varphi_m(X_{\tau}) = (h_m \circ  f)(X_{\tau})$ depends on a parameter $m \in \calM$.  For example, a call option on $Y=f(X)$  is obtained with $h^{\text{Call}}_m(y) := (y-m x_0)^{+}$ and $m$ is the \textit{moneyness} of the option.

The standard Monte Carlo  approach (MC) consists of simulating the underlying path  on a partition  $\Pi_{N} = \{0=t_0 < t_1 < ... < t_N=T \}$  that contains $\calT$ and compute the price as $p^{N,J}(m,\tau) = \frac{1}{J}\sum_{j=1}^J \varphi_m(X^{N,j}_\tau)$, $J\in \N$. 
In contrast, the \textit{Karhunen-Loève Monte Carlo method} (KLMC) samples $Y=f(X)$ directly and computes the price surface  using  the representation $p(m,\tau) = \E^{\Q}[ h_m(y_\tau)]$.  
We now describe the method in more depth. 
\subsection{The KLMC Algorithm}
We assume for simplicity that $Y$ has zero mean, otherwise minor changes  must be made for the KL expansion; see \cref{rem:center}. 
First, we simulate trajectories
$Y^j=(f(X_{t_n}^j))_{t_n \in \Pi_{N_{\text{off}}}}$, $j=1,...,J_{\text{off}}$ with  $J_{\text{off}}, N_{\text{off}} \in \N$,  and compute the eigenfunctions of $\kappa^{N_{\text{off}}}_Y$ as in  $\eqref{eq:eigendecomp}$. Next, for $k=1,...,K$,   we estimate the sample quantile function $\Phi_k^{-1}:[0,1]\to \R$ of $\xi_k = (Y,F_k)$ by employing method N\textsuperscript{o} $\! 7$ in \cite{HyndmanFan}.  
The coefficients $(\xi_k )$ can thereafter be simulated using inverse transform sampling \cite{Devroye}. 
  Notice that these steps can be done in an offline phase so $(\Phi_k^{-1})$ can be reused for other options contingent upon the functional $f$; see \Cref{sec:numResultX}. 
  

In the online phase, we simulate $J \in \N$ transformed paths using inverse transform sampling for $\xi_k$. This gives $y_{\tau}^{K,\frakF,j}= \sum_{k\le K}\xi_k^{j} F_k(\tau)$.  
Finally, the price surface is calculated using Monte Carlo.  The procedure is summarized in \Cref{alg:klmc}. 
It should be noted that although the coefficients $(\xi_k)$ are orthogonal in $L^2(\Q)$, they may well be \textit{dependent} when $Y$ is non-Gaussian. While the marginals of $(\xi_k)_{k\le K}$ are fitted properly, 
the dependence is omitted as generating dependent random vectors with unknown joint distribution is highly non-trivial. Nevertheless, this simplification doesn't induce a bias in the obtained prices as we shall see in the numerical experiments.  


\begin{algorithm}[t]
\caption{(KLMC) }\label{alg:klmc}
\begin{itemize}
\vspace{-2mm}
\item \textbf{Offline}: Given $f$, $K$,  $J_{\text{off}}$, $N_{\text{off}}$ 
\begin{enumerate}
\setlength \itemsep{0.2ex}
\vspace{-2mm}
\item Simulate trajectories $Y^j=(f(X_{t_n}^j))_{t_n \in \Pi_{N_{\text{off}}}}$, $j=1,...,J_{\text{off}}$
\item Compute $\kappa^{N_{\text{off}}}_Y$ (closed-form or from the sample  $(Y^j)$)
\item Solve the eigenvalue problem $\eqref{eq:eigendecomp}$ to obtain  $(\lambda^{\frakF}_k,F_k)$
\item Using $(Y^j)$, estimate the quantile functions  $\Phi_k^{-1}$, $k\le K$ 
\end{enumerate}
\vspace{-2mm}

\item \textbf{Online}: Given $J, \ \calM, \ \calT$ 
\begin{enumerate}
\setlength \itemsep{0.2ex}
\vspace{-2mm}
\item Simulate $\xi^j_k = \Phi^{-1}_k(u_k^j)$, $(u_k^j) \overset{i.i.d.}{\sim} U(0,1)$, $j \le J$, $k\le K$
\item Compute $y_\tau^{K,\frakF,j} =\sum_{k=1}^K  \xi^j_k    F_k(\tau)$, $\tau \in \calT$, $j\le J$ 
\item Estimate the price surface $p^{K,\frakF,J}(m,\tau) := \frac{1}{J}\sum_{j=1}^{J} h_m(y_\tau^{K,\frakF,j}).$
\end{enumerate}
\end{itemize}
\vspace{-3mm}
\end{algorithm}

\subsection{Numerical Results} \label{sec:numResultPrice}
First, we build the price surface for  Asian and lookback call options, i.e. by choosing $h_m = h^{\text{Call}}_m$ and the running maximum and time average as underlying functional, respectively. Of course, the put option price surface can  be retrieved thanks to  put-call parity. We also consider Up \& Out digital options, that is $f(X_t)=\max_{0\le s \le t}x_s$  and   $h^{\text{UO}}_m(y):=\mathds{1}_{\{y \ \le \ m x_0\}}$. The parameter $m\ge 1$ thus represents the barrier of the option relative to the spot price. 
We can therefore reuse the quantile functions computed for the lookback call options.

The parameters are $(x_0,\sigma,N_{\text{off}},J_{\text{off}},J) = (100, 0.2, 10^3, 2^{17}, 2^{19})$, 
 $T=1$ year and $\calT =  \{\frac{1}{52},\frac{2}{52},\ldots,1\}$ (weekly maturities). 
The moneyness and barrier levels are respectively $\calM^{\text{Call}} = \{0.75,0.80,\ldots,1.25\}$ and $\calM^{\text{UO}} = \{1.05,1.10,\ldots,1.50\}$. 
We assess  accuracy  in the mean square sense, namely by computing 
$$
\text{MSE} =  \frac{1}{|\calM|\ |\calT|}\sum_{(m,\tau) \in \calM \times \calT} |p^{\text{(B)}}(m,\tau) - \hat{p}(m,\tau)|^2. 
$$ The function $\hat{p}$ is the approximated price and $p^{\text{(B)}}$ a benchmark obtained using a standard Monte Carlo with $40 \cdot |\calT| = 2080$ time steps and same number of simulations. 
\interfootnotelinepenalty=10000 
  \cref{tab:results} displays the MSE, runtime (online phase) and number of variates per simulated path ($K$ and $N$ for the KLMC and MC method, respectively).\footnote{The experiments have been made on a personal computer; see \href{https://github.com/valentintissot/KLMC.git}{https://github.com/valentintissot/KLMC} for an implementation. The offline phase takes about 10 seconds per  functional.}  Notice that we increase $K,N$  for the lookback call and Up \& Out digital option as the running maximum has a slower rate of $L^2(\Q\otimes dt)$ convergence  as seen in \Cref{fig:L2Error} for the Brownian case.  The KLMC method constantly yields a lower MSE and runtime.  For the Asian call option, note that the number of variates per path is less than the number of maturity points and KLMC method, which couldn't be done with the MC method. 
  
 \vspace{-1mm}

\begin{table}[H]
    \centering
\caption{Mean squared errors and runtime (seconds)}
\vspace{-3mm}
\begin{tabular}{ccccccc} 
\hline  & \multicolumn{3}{c}{KLMC}  & \multicolumn{3}{c}{MC} \\  \hline
Option & K & MSE  & Time & N & MSE & Time \\
\hline  
 Asian Call & $40$ & 1.50e-04 & 2.26 & $ 52$ & 3.10e-04 & 2.40\\ 
Lookback Call   & $100$ & 1.15e-02 & 4.47& $4\cdot 52$ & 1.92e-01 & 7.05 \\ 
Up \& Out Digital   & $100$ & 1.80e-04 & 4.40 & $4\cdot 52$ & 1.90e-04 & 6.88\\ \hline
\end{tabular}
\label{tab:results}
\end{table}
